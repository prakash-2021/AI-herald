{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7DGbuYtB1JI",
        "outputId": "e8edccb4-5d35-4171-cb4d-7787d9fad9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function for Text Cleaning:\n",
        "\n",
        "Implement a Helper Function as per Text Preprocessing Notebook and Complete the following pipeline."
      ],
      "metadata": {
        "id": "SxV-QBHp-B6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Text Cleaning Pipeline"
      ],
      "metadata": {
        "id": "B-llg-TI_Drw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlKURi-cddbO",
        "outputId": "a1b6cb73-4f84-4d2b-de3f-f6e5a4740316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "tZqEV0wyi19z",
        "outputId": "e5ac7536-809a-4d4f-be2b-1e37b83b1222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def text_cleaning_pipeline(dataset, rule=\"lemmatize\"):\n",
        "    \"\"\"\n",
        "    A pipeline for text cleaning which includes converting to lowercase,\n",
        "    removing URLs, emojis, unwanted characters, stopwords, and performing either\n",
        "    lemmatization or stemming.\n",
        "\n",
        "    Args:\n",
        "    - dataset (str): Input text data to be cleaned.\n",
        "    - rule (str): Either 'lemmatize' or 'stem' to choose the text transformation method.\n",
        "\n",
        "    Returns:\n",
        "    - str: Cleaned text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the input to lowercase to standardize the text\n",
        "    data = dataset.lower()\n",
        "\n",
        "    # Remove URLs using a regex pattern\n",
        "    data = re.sub(r'http\\S+|www\\S+|https\\S+', '', data)\n",
        "\n",
        "    # Remove emojis using a regex pattern (removes non-ASCII characters)\n",
        "    data = re.sub(r'[^\\x00-\\x7F]+', '', data)\n",
        "\n",
        "    # Remove unwanted characters (non-alphanumeric characters except spaces)\n",
        "    data = re.sub(r'[^a-zA-Z0-9\\s]', '', data)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(data)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming or lemmatization\n",
        "    if rule == \"lemmatize\":\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    elif rule == \"stem\":\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "    else:\n",
        "        print(\"Pick between 'lemmatize' or 'stem' for the rule\")\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "KCT1T3-68tN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification using Machine Learning Models\n"
      ],
      "metadata": {
        "id": "hzMm4-1KCNkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Instructions: Trump Tweet Sentiment Classification\n",
        "\n",
        "1. **Load the Dataset**  \n",
        "   Load the dataset named `\"trump_tweet_sentiment_analysis.csv\"` using `pandas`. Ensure the dataset contains at least two columns: `\"text\"` and `\"label\"`.\n",
        "\n",
        "2. **Text Cleaning and Tokenization**  \n",
        "   Apply a text preprocessing pipeline to the `\"text\"` column. This should include:\n",
        "   - Lowercasing the text  \n",
        "   - Removing URLs, mentions, punctuation, and special characters  \n",
        "   - Removing stopwords  \n",
        "   - Tokenization (optional: stemming or lemmatization)\n",
        "   - \"Complete the above function\"\n",
        "\n",
        "3. **Train-Test Split**  \n",
        "   Split the cleaned and tokenized dataset into **training** and **testing** sets using `train_test_split` from `sklearn.model_selection`.\n",
        "\n",
        "4. **TF-IDF Vectorization**  \n",
        "   Import and use the `TfidfVectorizer` from `sklearn.feature_extraction.text` to transform the training and testing texts into numerical feature vectors.\n",
        "\n",
        "5. **Model Training and Evaluation**  \n",
        "   Import **Logistic Regression** (or any machine learning model of your choice) from `sklearn.linear_model`. Train it on the TF-IDF-embedded training data, then evaluate it using the test set.  \n",
        "   - Print the **classification report** using `classification_report` from `sklearn.metrics`.\n"
      ],
      "metadata": {
        "id": "oFltIxr9L2Wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Dataset**"
      ],
      "metadata": {
        "id": "eV7iQAt6fofO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Dataset**"
      ],
      "metadata": {
        "id": "YdsesTRgq25v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AI and ML/trum_tweet_sentiment_analysis.csv\")\n",
        "\n",
        "# Check the columns and if there are any missing values\n",
        "print(df.columns)\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyaWF1RQp-0D",
        "outputId": "d26ae360-2a96-4f7b-cb64-8074bbb4be25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'Sentiment'], dtype='object')\n",
            "text         0\n",
            "Sentiment    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning and Tokenization**"
      ],
      "metadata": {
        "id": "nMsq_s7VrEaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for text cleaning and preprocessing\n",
        "def text_cleaning_pipeline(dataset, rule=\"lemmatize\"):\n",
        "    # Lowercasing the text\n",
        "    data = dataset.lower()\n",
        "\n",
        "    # Remove URLs, mentions, punctuation, and special characters\n",
        "    data = re.sub(r'http\\S+|www\\S+|https\\S+', '', data)  # Remove URLs\n",
        "    data = re.sub(r'@\\w+', '', data)  # Remove mentions (@username)\n",
        "    data = re.sub(r'[^a-zA-Z0-9\\s]', '', data)  # Remove punctuation/special characters\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(data)\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply lemmatization or stemming\n",
        "    if rule == \"lemmatize\":\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    elif rule == \"stem\":\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "    else:\n",
        "        print(\"Pick between 'lemmatize' or 'stem' for the rule\")\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVtAeJHnrF6M",
        "outputId": "92be4a1e-dfd2-4f0c-8605-b25c1801344d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the cleaning function to the 'text' column\n",
        "df['cleaned_text'] = df['text'].apply(lambda x: text_cleaning_pipeline(x, rule='lemmatize'))\n",
        "\n",
        "# Check the cleaned data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3e3PcKYrKI3",
        "outputId": "3febbb22-d9b4-451b-d881-0592dc964ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  Sentiment  \\\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...          0   \n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...          0   \n",
            "2  Trump protests: LGBTQ rally in New York https:...          1   \n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...          0   \n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...          0   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  rt trump draining swamp taxpayer dollar trip a...  \n",
            "1  icymi hacker rig fm radio station play antitru...  \n",
            "2    trump protest lgbtq rally new york bbcworld via  \n",
            "3  hi im pier morgan david beckham awful donald t...  \n",
            "4  rt tech firm suing buzzfeed publishing unverif...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train-Test Split**"
      ],
      "metadata": {
        "id": "RINCF7wWrM5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare the data for training\n",
        "X = df['cleaned_text']  # Features\n",
        "y = df['Sentiment']  # Labels (sentiment)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "cAz0ZUHUrN7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorization**"
      ],
      "metadata": {
        "id": "D_-NxJoNrRjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)  # Transform test data\n"
      ],
      "metadata": {
        "id": "Y1NQvtWQrShH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "8aIfx4xErU50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)  # Train the model with the training data\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cCoZdg8rXhc",
        "outputId": "09644fd2-5aa8-4cc5-88c0-0f3a7340b00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97    248563\n",
            "           1       0.94      0.91      0.93    121462\n",
            "\n",
            "    accuracy                           0.95    370025\n",
            "   macro avg       0.95      0.94      0.95    370025\n",
            "weighted avg       0.95      0.95      0.95    370025\n",
            "\n"
          ]
        }
      ]
    }
  ]
}